\section{General Bias (what was lacking or what was less reliable/valid)}
%introduction
Here we will discuss possible biases in our project, with a focus on what biases that could effect the validity and reliability of our evaluation. We will first off discuss how the data that was used, was collected, under which condition the collection took place, and how the segmentation of the audio data for our program could have effected the results of the final evaluation of the data. Then we will discuss what data was the output from the program and what test has been made, then other methods of getting better data (other than the nominal data / frequency data that we got) will be discussed.

%how was the data collected, problems with the data set e.g noise, how it was segmented and target group. also manual seg vs auto seg.
The data collected for this project was an audio file that contains beatboxing from different participants. The participants were supposedly non-beatboxers\footnote{People who wouldn't regularly do beatboxing} and they were found using convenience sampling method, which means that the participants were chosen based on proximity. The fact that we did not consider their ability of beatboxing, could contribute to making the system and test less valid, because the way that the participants made beatboxing sounds, might not be similar to the sounds that beatboxers with experience make. The collection took place on AAU campus Copenhagen at A.C. Meyers VÃ¦nget 15 in the main hall. The location can contribute to some noise in the recordings, which might make the later classification different from recording at other places with less, more, or different noise. Next after the collection of the dataset, we needed to segment the audio, to make sound segments that contained the individual sounds of beatboxing\footnote{Unrecognized sounds or noise were annotated as such.}. 

This was done in two different ways: one where it was eye balled to find the segments\footnote{Using Sonic Visualiser}, and the other was automatically based on a thresholded audio feature (in our case the Root Mean Square (RMS)). The segments that the two methods returned, could be different in the preciseness of endpoints and duration of the sounds. This can influence the classification of the sounds, because one might contain more information than just the sound that we want to analyse. The second segmentation method (the automatic one), was not used for our tests, but would be used for future tests with new data. These differences might contribute to less validity in the classification of the sounds, but certainly also to less reliability.

%our test what we could have tested differently (we got frequency data any way that we could have gotten some kindof scale data instead perhaps with a test of executions speed, going out and make people try it to get there input into how correct they felt that their beatboxing was transcribed)

For the evaluation of our program, confusion matrices were made that shows the accuracy of the classifier with various settings and different features. Secondly to make a statistical evaluation of the results, we made use of the chi squared test. The reason we used a chi squared test, was that the data we collected from the classification of beatboxing, the matrices, was categorized, and by that they are nominal data. To change that, instead of focusing on how accurate the system was, we could have made a test of how fast the classification was executed with the different audio features. This would give us different timings instead of categories, which would be ratio data, which can give us more opportunities of good statistical testing (i.e. we get parametric data). A test that could be done on these different measurements, would answer another question than the one we asked. We chose to use our test design, because it would give a better picture of how the system performed with the classifier and how it could perform better. This would better answer if we made a good transcription system, compared to a timing of the speed, although that could be relevant, if the aim was a system capable of fast classification (e.g. real-time systems).
%sum up


To recapitulate: there are a few changes that could be made to make the experiment more valid and reliable, but the overall validity of the experiment can be considered to be sufficient. For the reliability we can say that the experiment should be possible to recreate, if a dataset like our was collected, or if our dataset was provided. Differences could perhaps occur, if some aspects of the implementation differed in any matter.